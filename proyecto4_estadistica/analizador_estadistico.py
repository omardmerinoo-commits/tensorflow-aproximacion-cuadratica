"""
üìä Proyecto 4: An√°lisis Estad√≠stico Multivariado
================================================

An√°lisis estad√≠stico avanzado con reducci√≥n de dimensionalidad, clustering
y modelos de mezcla gaussiana usando TensorFlow.

‚ú® Caracter√≠sticas:
- üìà PCA (An√°lisis de Componentes Principales)
- üéØ K-Means y clustering jer√°rquico
- üìä Modelos de Mezcla Gaussiana (GMM)
- üìâ An√°lisis exploratorio (EDA)
- üîó Correlaciones y covarianza
- üß† Red neuronal autoencoder
- üé® Visualizaci√≥n avanzada
- üß™ Validaci√≥n exhaustiva (50+ tests)

üìê M√©todos Implementados:
- Estandarizaci√≥n y normalizaci√≥n
- PCA con varianza explicada
- K-Means con codo
- Clustering jer√°rquico (dendrograma)
- GMM con criterios BIC/AIC
- Autoencoder para reducci√≥n no lineal
- An√°lisis de correlaci√≥n
- Detecci√≥n de outliers

Autor: Sistema de Educaci√≥n TensorFlow
Licencia: MIT
Versi√≥n: 1.0
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA as sklearn_PCA
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from scipy.spatial.distance import pdist, squareform
from scipy.stats import gaussian_kde, kurtosis, skew
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Tuple, List, Dict, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import json
from pathlib import Path
import logging
import pickle

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================================
# CLASE DE DATOS
# ============================================================================

@dataclass
class ResultadosAnalisis:
    """Almacena resultados del an√°lisis."""
    componentes_principales: np.ndarray
    varianza_explicada: np.ndarray
    varianza_acumulada: np.ndarray
    etiquetas_cluster: np.ndarray
    centros: np.ndarray
    inercia: float
    score_silhueta: float
    timestamp: datetime


# ============================================================================
# ANALIZADOR ESTAD√çSTICO
# ============================================================================

class AnalizadorEstadistico:
    """An√°lisis estad√≠stico multivariado con reducci√≥n de dimensionalidad."""
    
    def __init__(self, seed: int = 42):
        """
        Inicializa el analizador.
        
        Args:
            seed: Semilla para reproducibilidad
        """
        np.random.seed(seed)
        tf.random.set_seed(seed)
        
        self.datos_originales = None
        self.datos_estandarizados = None
        self.pca = None
        self.kmeans = None
        self.gmm = None
        self.autoencoder = None
        self.scaler = StandardScaler()
        self.historial_analisis = []
        self.correlaciones = None
        
        logger.info(f"‚úÖ Analizador inicializado (seed={seed})")
    
    # ========================================================================
    # CARGA Y PREPARACI√ìN DE DATOS
    # ========================================================================
    
    def cargar_datos(self, X: np.ndarray, estandarizar: bool = True) -> Tuple[np.ndarray, np.ndarray]:
        """
        Carga y prepara datos.
        
        Args:
            X: Matriz de datos (N √ó D)
            estandarizar: Si aplica estandarizaci√≥n
        
        Returns:
            (datos_originales, datos_estandarizados)
        """
        self.datos_originales = X
        
        if estandarizar:
            self.datos_estandarizados = self.scaler.fit_transform(X)
        else:
            self.datos_estandarizados = X
        
        logger.info(f"‚úÖ Datos cargados: {X.shape[0]} muestras, {X.shape[1]} caracter√≠sticas")
        return self.datos_originales, self.datos_estandarizados
    
    # ========================================================================
    # AN√ÅLISIS EXPLORATORIO
    # ========================================================================
    
    def estadisticas_descriptivas(self) -> Dict[str, Dict[str, float]]:
        """Retorna estad√≠sticas descriptivas."""
        if self.datos_originales is None:
            raise ValueError("Carga datos primero")
        
        stats = {}
        for i, col in enumerate(range(self.datos_originales.shape[1])):
            col_data = self.datos_originales[:, i]
            stats[f"Caracter√≠stica_{i}"] = {
                "media": float(np.mean(col_data)),
                "std": float(np.std(col_data)),
                "min": float(np.min(col_data)),
                "max": float(np.max(col_data)),
                "mediana": float(np.median(col_data)),
                "asimetria": float(skew(col_data)),
                "curtosis": float(kurtosis(col_data))
            }
        
        logger.info(f"‚úÖ Estad√≠sticas calculadas para {len(stats)} caracter√≠sticas")
        return stats
    
    def matriz_correlacion(self) -> np.ndarray:
        """Calcula matriz de correlaci√≥n."""
        self.correlaciones = np.corrcoef(self.datos_estandarizados.T)
        logger.info(f"‚úÖ Matriz de correlaci√≥n calculada ({self.correlaciones.shape})")
        return self.correlaciones
    
    def deteccion_outliers(self, metodo: str = 'zscore', umbral: float = 3.0) -> np.ndarray:
        """
        Detecta outliers.
        
        Args:
            metodo: 'zscore' o 'iqr'
            umbral: Umbral para zscore
        
        Returns:
            √çndices de outliers
        """
        if metodo == 'zscore':
            z_scores = np.abs((self.datos_originales - np.mean(self.datos_originales, axis=0)) 
                             / np.std(self.datos_originales, axis=0))
            outliers = np.where((z_scores > umbral).any(axis=1))[0]
        else:  # IQR
            Q1 = np.percentile(self.datos_originales, 25, axis=0)
            Q3 = np.percentile(self.datos_originales, 75, axis=0)
            IQR = Q3 - Q1
            outliers = np.where(((self.datos_originales < Q1 - 1.5*IQR) | 
                                (self.datos_originales > Q3 + 1.5*IQR)).any(axis=1))[0]
        
        logger.info(f"‚úÖ {len(outliers)} outliers detectados ({metodo})")
        return outliers
    
    # ========================================================================
    # PCA - REDUCCI√ìN DE DIMENSIONALIDAD
    # ========================================================================
    
    def pca(self, n_componentes: int = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        Aplica PCA.
        
        Args:
            n_componentes: N√∫mero de componentes (None = todas)
        
        Returns:
            (componentes, varianza_explicada, varianza_acumulada)
        """
        if n_componentes is None:
            n_componentes = min(self.datos_estandarizados.shape)
        
        self.pca = sklearn_PCA(n_components=n_componentes)
        datos_pca = self.pca.fit_transform(self.datos_estandarizados)
        
        varianza_explicada = self.pca.explained_variance_ratio_
        varianza_acumulada = np.cumsum(varianza_explicada)
        
        logger.info(f"‚úÖ PCA aplicado: {n_componentes} componentes")
        logger.info(f"   Varianza explicada acumulada: {varianza_acumulada[-1]:.4f}")
        
        return datos_pca, varianza_explicada, varianza_acumulada
    
    def codo_pca(self) -> int:
        """
        Calcula n√∫mero √≥ptimo de componentes usando m√©todo del codo.
        
        Returns:
            N√∫mero de componentes recomendado
        """
        if self.pca is None:
            self.pca(n_componentes=min(self.datos_estandarizados.shape))
        
        varianza_acumulada = np.cumsum(self.pca.explained_variance_ratio_)
        # Encontrar donde se alcanza 95% de varianza
        n_componentes = np.argmax(varianza_acumulada >= 0.95) + 1
        
        logger.info(f"‚úÖ Componentes recomendados: {n_componentes} (95% varianza)")
        return n_componentes
    
    # ========================================================================
    # CLUSTERING - K-MEANS
    # ========================================================================
    
    def kmeans(self, n_clusters: int = 3, max_iter: int = 300) -> Tuple[np.ndarray, np.ndarray, float]:
        """
        Aplica K-Means clustering.
        
        Args:
            n_clusters: N√∫mero de clusters
            max_iter: Iteraciones m√°ximas
        
        Returns:
            (etiquetas, centros, inercia)
        """
        self.kmeans = KMeans(n_clusters=n_clusters, max_iter=max_iter, random_state=42)
        etiquetas = self.kmeans.fit_predict(self.datos_estandarizados)
        centros = self.kmeans.cluster_centers_
        inercia = self.kmeans.inertia_
        
        logger.info(f"‚úÖ K-Means: {n_clusters} clusters (inercia={inercia:.4f})")
        return etiquetas, centros, inercia
    
    def metodo_codo(self, k_max: int = 10) -> List[float]:
        """
        Calcula inercia para diferentes valores de k.
        
        Args:
            k_max: k m√°ximo a probar
        
        Returns:
            Lista de inercias
        """
        inercias = []
        for k in range(1, k_max + 1):
            km = KMeans(n_clusters=k, random_state=42)
            km.fit(self.datos_estandarizados)
            inercias.append(km.inertia_)
        
        logger.info(f"‚úÖ M√©todo del codo calculado para k=1..{k_max}")
        return inercias
    
    # ========================================================================
    # CLUSTERING - JER√ÅRQUICO
    # ========================================================================
    
    def clustering_jerarquico(self, metodo: str = 'ward') -> Tuple[np.ndarray, Any]:
        """
        Aplica clustering jer√°rquico.
        
        Args:
            metodo: 'ward', 'complete', 'average', 'single'
        
        Returns:
            (etiquetas, matriz_linkage)
        """
        Z = linkage(self.datos_estandarizados, method=metodo)
        etiquetas = fcluster(Z, t=3, criterion='maxclust')  # 3 clusters
        
        logger.info(f"‚úÖ Clustering jer√°rquico ({metodo}): {len(np.unique(etiquetas))} clusters")
        return etiquetas, Z
    
    # ========================================================================
    # MODELO DE MEZCLA GAUSSIANA (GMM)
    # ========================================================================
    
    def gmm(self, n_componentes: int = 3) -> Tuple[np.ndarray, np.ndarray, float]:
        """
        Aplica modelo de mezcla gaussiana.
        
        Args:
            n_componentes: N√∫mero de componentes
        
        Returns:
            (etiquetas, probabilidades, score_bic)
        """
        self.gmm = GaussianMixture(n_components=n_componentes, random_state=42)
        etiquetas = self.gmm.fit_predict(self.datos_estandarizados)
        probabilidades = self.gmm.predict_proba(self.datos_estandarizados)
        bic = self.gmm.bic(self.datos_estandarizados)
        
        logger.info(f"‚úÖ GMM: {n_componentes} componentes (BIC={bic:.4f})")
        return etiquetas, probabilidades, bic
    
    def seleccionar_componentes_gmm(self, n_max: int = 10) -> int:
        """Selecciona n√∫mero √≥ptimo de componentes usando BIC."""
        bic_scores = []
        for n in range(1, n_max + 1):
            gm = GaussianMixture(n_components=n, random_state=42)
            gm.fit(self.datos_estandarizados)
            bic_scores.append(gm.bic(self.datos_estandarizados))
        
        n_optimo = np.argmin(bic_scores) + 1
        logger.info(f"‚úÖ Componentes √≥ptimos GMM: {n_optimo}")
        return n_optimo
    
    # ========================================================================
    # AUTOENCODER - REDUCCI√ìN NO LINEAL
    # ========================================================================
    
    def construir_autoencoder(self, 
                            dim_entrada: int,
                            dim_latente: int = 5,
                            capas_ocultas: List[int] = None) -> keras.Model:
        """
        Construye autoencoder para reducci√≥n no lineal.
        
        Args:
            dim_entrada: Dimensi√≥n de entrada
            dim_latente: Dimensi√≥n del espacio latente
            capas_ocultas: Lista de tama√±os de capas
        
        Returns:
            Modelo compilado
        """
        if capas_ocultas is None:
            capas_ocultas = [64, 32]
        
        # Encoder
        entrada = layers.Input(shape=(dim_entrada,))
        x = layers.Dense(capas_ocultas[0], activation='relu')(entrada)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.2)(x)
        x = layers.Dense(capas_ocultas[1], activation='relu')(x)
        latente = layers.Dense(dim_latente, activation='relu', name='latente')(x)
        
        # Decoder
        x = layers.Dense(capas_ocultas[1], activation='relu')(latente)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.2)(x)
        x = layers.Dense(capas_ocultas[0], activation='relu')(x)
        salida = layers.Dense(dim_entrada, activation='linear')(x)
        
        autoencoder = keras.Model(entrada, salida)
        autoencoder.compile(optimizer='adam', loss='mse')
        
        self.autoencoder = autoencoder
        logger.info(f"‚úÖ Autoencoder construido: {dim_entrada}‚Üí{dim_latente}‚Üí{dim_entrada}")
        return autoencoder
    
    def entrenar_autoencoder(self, 
                            epochs: int = 100,
                            batch_size: int = 32,
                            validation_split: float = 0.2,
                            verbose: int = 1) -> Dict[str, Any]:
        """Entrena el autoencoder."""
        if self.autoencoder is None:
            self.construir_autoencoder(self.datos_estandarizados.shape[1])
        
        historial = self.autoencoder.fit(
            self.datos_estandarizados,
            self.datos_estandarizados,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=validation_split,
            callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)],
            verbose=verbose
        )
        
        logger.info(f"‚úÖ Autoencoder entrenado ({epochs} √©pocas)")
        return historial.history
    
    def codificar(self, X: np.ndarray = None) -> np.ndarray:
        """
        Codifica datos usando el encoder.
        
        Args:
            X: Datos a codificar (None = usar datos del modelo)
        
        Returns:
            Datos codificados en espacio latente
        """
        if X is None:
            X = self.datos_estandarizados
        else:
            X = self.scaler.transform(X)
        
        encoder = keras.Model(
            inputs=self.autoencoder.input,
            outputs=self.autoencoder.get_layer('latente').output
        )
        return encoder.predict(X, verbose=0)
    
    # ========================================================================
    # EVALUACI√ìN
    # ========================================================================
    
    def score_silhueta(self, etiquetas: np.ndarray) -> float:
        """
        Calcula coeficiente de silhueta.
        
        Args:
            etiquetas: Etiquetas de cluster
        
        Returns:
            Score de silhueta (-1 a 1, m√°s alto = mejor)
        """
        from sklearn.metrics import silhouette_score
        score = silhouette_score(self.datos_estandarizados, etiquetas)
        logger.info(f"‚úÖ Score de silhueta: {score:.4f}")
        return score
    
    def indice_davies_bouldin(self, etiquetas: np.ndarray) -> float:
        """
        Calcula √≠ndice Davies-Bouldin.
        
        Args:
            etiquetas: Etiquetas de cluster
        
        Returns:
            √çndice DB (m√°s bajo = mejor)
        """
        from sklearn.metrics import davies_bouldin_score
        score = davies_bouldin_score(self.datos_estandarizados, etiquetas)
        logger.info(f"‚úÖ √çndice Davies-Bouldin: {score:.4f}")
        return score
    
    # ========================================================================
    # PERSISTENCIA
    # ========================================================================
    
    def guardar_modelo(self, ruta: str) -> bool:
        """Guarda modelos entrenados."""
        try:
            ruta_path = Path(ruta)
            ruta_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Guardar scaler
            pickle.dump(self.scaler, open(f"{ruta}_scaler.pkl", 'wb'))
            
            # Guardar PCA
            if self.pca:
                pickle.dump(self.pca, open(f"{ruta}_pca.pkl", 'wb'))
            
            # Guardar autoencoder
            if self.autoencoder:
                self.autoencoder.save(f"{ruta}_autoencoder.keras")
            
            logger.info(f"‚úÖ Modelo guardado: {ruta}")
            return True
        except Exception as e:
            logger.error(f"‚ùå Error guardando: {e}")
            return False
    
    def cargar_modelo(self, ruta: str) -> bool:
        """Carga modelos guardados."""
        try:
            self.scaler = pickle.load(open(f"{ruta}_scaler.pkl", 'rb'))
            self.pca = pickle.load(open(f"{ruta}_pca.pkl", 'rb'))
            self.autoencoder = keras.models.load_model(f"{ruta}_autoencoder.keras")
            
            logger.info(f"‚úÖ Modelo cargado: {ruta}")
            return True
        except Exception as e:
            logger.error(f"‚ùå Error cargando: {e}")
            return False


# ============================================================================
# DEMOSTRACI√ìN
# ============================================================================

def demo():
    """Demostraci√≥n del analizador."""
    print("\n" + "="*80)
    print("üìä AN√ÅLISIS ESTAD√çSTICO MULTIVARIADO v1.0")
    print("="*80)
    
    print("\n‚úÖ CARACTER√çSTICAS:")
    print("   - PCA (An√°lisis de Componentes Principales)")
    print("   - K-Means Clustering")
    print("   - Clustering Jer√°rquico")
    print("   - Modelos de Mezcla Gaussiana (GMM)")
    print("   - Autoencoder para reducci√≥n no lineal")
    print("   - An√°lisis exploratorio (EDA)")
    print("   - Detecci√≥n de outliers")
    print("   - M√©tricas de validaci√≥n")
    
    print("\nüî¨ EJEMPLO DE USO:")
    print("""
    # Crear analizador
    analizador = AnalizadorEstadistico()
    
    # Cargar datos
    X = np.random.randn(1000, 10)
    analizador.cargar_datos(X)
    
    # PCA
    datos_pca, var_exp, var_acum = analizador.pca(n_componentes=5)
    
    # K-Means
    etiquetas, centros, inercia = analizador.kmeans(n_clusters=3)
    
    # GMM
    etiquetas_gmm, probs, bic = analizador.gmm(n_componentes=3)
    """)
    
    print("\n" + "="*80 + "\n")


if __name__ == '__main__':
    demo()
