# Proyecto 6: Aproximador de Funciones No-Lineales
## AproximaciÃ³n Universal de Funciones MatemÃ¡ticas con Redes Neuronales

---

## ðŸŽ¯ IntroducciÃ³n

El **Aproximador de Funciones No-Lineales** demuestra el **Teorema de AproximaciÃ³n Universal** de redes neuronales.
Entrena modelos para aprender automÃ¡ticamente sin(x), cos(x), exp(x), xÂ³, xâµ y combinaciones.

**CaracterÃ­sticas**:
- NormalizaciÃ³n avanzada (StandardScaler + MinMaxScaler)
- RegularizaciÃ³n (L1, L2, dropout)
- Batch normalization
- Learning rate scheduling
- Arquitecturas mÃºltiples (MLP, Residual)
- >90% de precisiÃ³n en aproximaciÃ³n

---

## ðŸ“š TeorÃ­a MatemÃ¡tica

### Teorema de AproximaciÃ³n Universal (UAT)

Toda funciÃ³n continua acotada $f: [a,b] \rightarrow \mathbb{R}$ puede ser aproximada uniformemente
por una red neuronal artificial con una capa oculta:

$$\forall \epsilon > 0, \exists N \in \mathbb{N}, \mathbf{w}, \mathbf{b} : \|\mathbf{f}(\mathbf{x}) - \hat{\mathbf{f}}(\mathbf{x})\|_\infty < \epsilon$$

Donde $\hat{\mathbf{f}}$ es la salida de la red con $N$ unidades y pesos $\mathbf{w}$, sesgos $\mathbf{b}$.

### Funciones Aproximadas

#### 1. FunciÃ³n Sinusoidal: $f(x) = \sin(x)$
- **Dominio**: $[-2\pi, 2\pi]$
- **Rango**: $[-1, 1]$
- **CaracterÃ­sticas**: PeriÃ³dica, suave, oscilante
- **DesafÃ­o**: Capturar periodicidad

$$\sin(x) = \sum_{n=0}^{\infty} \frac{(-1)^n x^{2n+1}}{(2n+1)!}$$

#### 2. FunciÃ³n Exponencial: $f(x) = e^x$
- **Dominio**: $[-2, 2]$
- **Rango**: $[e^{-2}, e^2] \approx [0.135, 7.389]$
- **CaracterÃ­sticas**: Crecimiento exponencial, asimÃ©trica
- **DesafÃ­o**: DinÃ¡mica rÃ¡pida en rango positivo

$$e^x = \sum_{n=0}^{\infty} \frac{x^n}{n!}$$

#### 3. Polinomios: $f(x) = x^n$
- **$x^3$**: Dominio $[-2, 2]$, rango $[-8, 8]$
- **$x^5$**: Dominio $[-1.5, 1.5]$, rango $[-7.59, 7.59]$
- **CaracterÃ­sticas**: No acotados, monotÃ³nicos fuera origen
- **DesafÃ­o**: Capturar comportamiento no-lineal

---

## ðŸ› ï¸ TecnologÃ­as

| Componente | VersiÃ³n |
|------------|---------|
| TensorFlow | 2.16.0+ |
| scikit-learn | 1.3.0+ |
| NumPy | 1.24.0+ |

---

## ðŸ“¦ InstalaciÃ³n

```bash
cd proyecto6_funciones
pip install -r requirements.txt
```

---

## ðŸ—ï¸ Arquitecturas

### MLP EstÃ¡ndar

```
Entrada (1 caracterÃ­stica)
        â†“
Dense(128) + ReLU + BatchNorm + Dropout(0.3)
        â†“
Dense(64) + ReLU + BatchNorm + Dropout(0.3)
        â†“
Dense(32) + ReLU + BatchNorm + Dropout(0.3)
        â†“
Dense(1, Linear)
```

**ParÃ¡metros**: ~12K
**Ventajas**: Simple, rÃ¡pido, convergencia estable

### Red Residual

```
Entrada (1)
    â†“
Dense(64) + ReLU + BatchNorm + Dropout(0.2)
    â†“
Dense(32) + ReLU + BatchNorm + Dropout(0.2) â”€â”€â”
    â†“                                           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Suma (Skip Connection) â†â”€â”€â”€â”€â”˜
    â†“
Dense(1, Linear)
```

**Ventajas**: Mejor para redes profundas, evita vanishing gradients

---

## ðŸ“– GuÃ­a de Uso

### Generar Datos

```python
from aproximador_funciones import GeneradorFuncionesNoLineales

generador = GeneradorFuncionesNoLineales()

# sin(x)
datos_sin = generador.generar('sin', n_muestras=500, ruido=0.05)

# exp(x)
datos_exp = generador.generar('exp', n_muestras=500)

# xÂ³
datos_x3 = generador.generar('x3', n_muestras=500)
```

### Entrenar Modelo

```python
from aproximador_funciones import AproximadorFuncion

aprox = AproximadorFuncion()

historial = aprox.entrenar(
    datos_sin.X_train, datos_sin.y_train,
    datos_sin.X_test, datos_sin.y_test,
    epochs=100,
    arquitectura='mlp'  # o 'residual'
)
```

### Evaluar

```python
metricas = aprox.evaluar(datos_sin.X_test, datos_sin.y_test)
print(f"RÂ²: {metricas['r2_score']:.4f}")
print(f"RMSE: {metricas['rmse']:.6f}")
```

### Predecir

```python
import numpy as np

X_nuevo = np.array([0.5, 1.0, 1.5]).reshape(-1, 1)
y_pred = aprox.predecir(X_nuevo)
```

---

## ðŸ§ª Suite de Pruebas

```bash
pytest test_aproximador_funciones.py -v --cov
```

**70+ tests** cubriendo:
- GeneraciÃ³n de 6 funciones diferentes
- NormalizaciÃ³n (entrada/salida)
- Arquitecturas (MLP, Residual)
- RegularizaciÃ³n (L1, L2)
- Entrenamiento y convergencia
- EvaluaciÃ³n con 5 mÃ©tricas
- PredicciÃ³n
- Persistencia

---

## ðŸ“Š Resultados Esperados

### PrecisiÃ³n por FunciÃ³n

| FunciÃ³n | RÂ² | RMSE |
|---------|-----|------|
| sin(x) | >0.95 | <0.05 |
| cos(x) | >0.95 | <0.05 |
| exp(x) | >0.98 | <0.02 |
| xÂ³ | >0.99 | <0.01 |
| xâµ | >0.99 | <0.01 |
| sinÂ·cos | >0.90 | <0.10 |

### Convergencia

- **Ã‰pocas tÃ­picas**: 30-50
- **Tiempo de entrenamiento**: 5-10 segundos
- **Tiempo de predicciÃ³n**: <1ms/muestra

---

## ðŸŽ“ ConclusiÃ³n

Este proyecto valida matemÃ¡ticamente la capacidad de redes neuronales para aproximar
cualquier funciÃ³n continua, fundamental en deep learning moderno.

**Impacto**: Base teÃ³rica para regression, forecasting, y aproximaciÃ³n universal.

---

**Status**: âœ… Production Ready | **Tests**: >90% | **Docs**: Completa
